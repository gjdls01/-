{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1a2J3rXYpkp6"
      },
      "outputs": [],
      "source": [
        "!set -x \\\n",
        "&& pip install konlpy \\\n",
        "&& curl -s https://raw.githubusercontent.com/konlpy/konlpy/master/scripts/mecab.sh | bash -x"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.lm import NgramCounter\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "from nltk.lm.preprocessing import padded_everygram_pipeline\n",
        "from nltk.lm import MLE\n",
        "import json\n",
        "import konlpy\n",
        "from nltk.tokenize import word_tokenize\n",
        "import re\n",
        "from konlpy.tag import Okt\n",
        "from konlpy.tag import Mecab, Komoran\n",
        "import time\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "from tqdm import tqdm\n",
        "from itertools import combinations\n",
        "from itertools import product\n",
        "with open(\"/content/gdrive/MyDrive/origin/BWSC217000049025.json\", \"r\") as f:\n",
        "\n",
        "    json_data=json.load(f)\n",
        "#print(json.dumps(json_data))\n",
        "kor_text=json_data[\"SJML\"][\"text\"][1][\"content\"]\n",
        "type(kor_text)"
      ],
      "metadata": {
        "id": "xK3HQwnYpt0v",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "92ac9c8c-3c15-40a6-e924-9d7f079c008b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "str"
            ]
          },
          "metadata": {},
          "execution_count": 119
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pattern_1=r\"\\([^)]*\\)\"\n",
        "pattern_2=r\"\\.{2}\"\n",
        "def preprocess_sent(lst_text):\n",
        "    wholelist=[]\n",
        "    buf=str()\n",
        "    subkor_1=re.sub(pattern=pattern_1, repl=\"\", string=lst_text)\n",
        "    subkor_2=re.sub(pattern=pattern_2, repl=\"\", string=subkor_1)\n",
        "    clear_list=re.compile(\"[가-힣]+|\\.{1}\").findall(subkor_2)\n",
        "    for word in clear_list:\n",
        "        if word==\".\":\n",
        "            buf+=(word+\" \")\n",
        "            wholelist.append(buf)\n",
        "            buf=str()\n",
        "        \n",
        "        else:\n",
        "            buf+=(word+\" \")\n",
        "    return wholelist\n",
        "\n",
        "komoran=Komoran()\n",
        "mecab=Mecab()\n",
        "lst_sent=preprocess_sent(kor_text)\n",
        "def sent_to_morph(lst_sent):\n",
        "    double_list=[]\n",
        "    for sentence in lst_sent:\n",
        "        #listed=komoran.morphs(sentence)\n",
        "        listed=mecab.morphs(sentence)\n",
        "        double_list.append(listed)\n",
        "    return double_list"
      ],
      "metadata": {
        "id": "9JIvLR01qYhB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "listed=preprocess_sent(kor_text)\n",
        "double=sent_to_morph(listed)\n",
        "print(double)"
      ],
      "metadata": {
        "id": "XhdLGEcjrKAG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "58c2267f-32ec-41ef-9a02-4fd758583bab"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[['회장', '은', '일', '어려운', '시기', '에', '뽑', '아', '줘서', '막중', '한', '책임', '을', '느낀다', '며', '통신', '대표', '기업', '등', '를', '만들', '겠', '다', '며', '포부', '를', '밝혔', '다', '.'], ['황', '회장', '은', '이날', '서초구', '우면동', '연구개발', '센터', '에서', '열린', '주주', '총회', '에', '정식', '회장', '으로', '선임', '된', '후', '인사말', '에서', '이', '같이', '밝혔', '다', '.'], ['그', '는', '최고', '의', '품질', '과', '차별', '화', '된', '서비스', '를', '먼저', '선보이', '고', '기반', '의', '새로운', '성장', '엔진', '을', '만들', '겠', '다', '며', '의', '성공', '스토리', '로', '글로벌', '시장', '을', '개척', '하', '겠', '다', '고', '말', '했', '다', '.']]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#학습된 데이터 바탕으로 모델\n",
        "def bigram_probs(model, pre_word, post_word):\n",
        "    word_dict=dict(model.counts[[pre_word]])\n",
        "    wholecount=model.counts[pre_word]\n",
        "    if wholecount!=0:\n",
        "\n",
        "      try:\n",
        "          post_count=word_dict[post_word]\n",
        "      except:\n",
        "          post_count=0\n",
        "     \n",
        "\n",
        "      emer_prob=post_count/wholecount\n",
        "\n",
        "    else:\n",
        "      emer_prob=0\n",
        "    return emer_prob\n",
        "\n",
        "def trigram_probs(model, pre_word, post_word):\n",
        "    tulist=[]\n",
        "    word_dict=dict(model.counts[[pre_word]])\n",
        "    \n",
        "    for key in list(word_dict.keys()):\n",
        "        if key !=pre_word:\n",
        "            first_prob=bigram_probs(model, pre_word, key)\n",
        "            second_prob=bigram_probs(model, key, post_word)\n",
        "            whole_prob=first_prob*second_prob\n",
        "            if whole_prob!=0:\n",
        "                #normalization\n",
        "                whole_prob=whole_prob*2\n",
        "                tulist.append([pre_word, key, post_word, whole_prob])\n",
        "        else:\n",
        "            continue\n",
        "    return tulist"
      ],
      "metadata": {
        "id": "F8pGla0EtnFH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#bigram, trigram이 모두 되는지 확인해보기. \n",
        "def canconnect(model, pre_word, post_word):\n",
        "    possiblelist=[]\n",
        "    biprob=bigram_probs(model, pre_word, post_word)\n",
        "    if biprob!=0:\n",
        "        possiblelist.append([pre_word, post_word, biprob])\n",
        "    \n",
        "    tulist=trigram_probs(model, pre_word, post_word)\n",
        "    for li in tulist:\n",
        "        possiblelist.append(li)\n",
        "    return possiblelist\n",
        "  \n",
        "#  가능한 케이스들과 이에 대한 확률 제시. 인덱스도 만들어야 해(가능한 경우의 수 개수)\n",
        "def bigconnect(model, list_of_words):\n",
        "    dummylist=[]\n",
        "    index_list=[]\n",
        "    for i in range(len(list_of_words)-1):\n",
        "        connection=canconnect(model, list_of_words[i], list_of_words[i+1])\n",
        "        if len(connection)!=0:\n",
        "            n=len(connection)\n",
        "            index_list.append(n)\n",
        "            for possible in connection:\n",
        "                dummylist.append(possible)\n",
        "    \n",
        "        else:\n",
        "            print(\"connection failed\")\n",
        "            break\n",
        "    return dummylist, index_list\n"
      ],
      "metadata": {
        "id": "5WasIdhivt2V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text=[[\"a\", \"b\", \"c\", \"d\"], [\"가\", \"다\", \"a\", \"b\"], [\"a\", \"d\", \"b\", \"c\"], [\"c\", \"d\", \"b\", \"e\", \"a\"], [\"c\", \"b\", \"a\"], [\"d\", \"c\"]]\n",
        "train, vocab=padded_everygram_pipeline(2, text)\n",
        "vocab=list(vocab)\n",
        "ngram=MLE(3)\n",
        "ngram.fit(train,vocab)"
      ],
      "metadata": {
        "id": "6T_tJdrvv35R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bigconnect(ngram, [\"c\",  \"a\", \"d\", \"e\"])\n"
      ],
      "metadata": {
        "id": "ZW1kYblFwCvc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "90090933-e2c0-4e21-edf1-5ad0c3ba6c14"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "([['c', 'b', 'a', 0.08000000000000002], ['a', 'd', 0.2], ['d', 'b', 'e', 0.2]],\n",
              " [1, 1, 1])"
            ]
          },
          "metadata": {},
          "execution_count": 101
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def process(model, list_of_words):\n",
        "    doublelists, index=bigconnect(model, list_of_words)\n",
        "    for indexes in range(len(doublelists)):\n",
        "        doublelists[indexes]=tuple(doublelists[indexes])\n",
        "    return doublelists, index\n",
        "\n",
        "def separate_connection(model,list_of_words):\n",
        "    newconlist=[]\n",
        "    pro_doublelists, pro_index=process(model, list_of_words)\n",
        "    end, start=0, 0\n",
        "    for i in pro_index:\n",
        "        end+=i\n",
        "        newconlist.append(pro_doublelists[start:end])\n",
        "\n",
        "        start+=i\n",
        "    return newconlist\n",
        "  \n",
        "def make_it_one(model, list_of_words):\n",
        "    wholeconcat=[]\n",
        "    wholeprob=1.0\n",
        "    newlist=separate_connection(model, list_of_words)\n",
        "    maybe_one=list(product(*newlist))\n",
        "    for i in maybe_one:\n",
        "        for index in range(len(i)):\n",
        "            wholeconcat.append(i[index][:-2])\n",
        "            wholeprob*=i[index][-1]\n",
        "        wholeconcat.append(list_of_words[-1])\n",
        "        \n",
        "        wholeconcat.append(wholeprob)\n",
        "        wholeprob=1.0\n",
        "    return wholeconcat\n"
      ],
      "metadata": {
        "id": "Yk03fhe6wFKq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "make_it_one(ngram, [\"c\", \"a\", \"b\"])"
      ],
      "metadata": {
        "id": "6FuuwkyCwSGC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "123f764b-460c-4fad-aed1-cbd1c8825ab0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('c', 'b'),\n",
              " ('a',),\n",
              " 'b',\n",
              " 0.03200000000000001,\n",
              " ('c', 'b'),\n",
              " ('a', 'd'),\n",
              " 'b',\n",
              " 0.016000000000000004]"
            ]
          },
          "metadata": {},
          "execution_count": 103
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#이제부터는 학습 phase. 일단 주어진 텍스트 활용해서 ngram model 학습위주만"
      ],
      "metadata": {
        "id": "CTZp-Z3LwZKP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "d_path=\"/content/gdrive/MyDrive/origin\"\n",
        "filelist=os.listdir(d_path)\n",
        "for filename in filelist:\n",
        "    uniquefile=os.path.join(d_path, filename )\n",
        "\n",
        "\n",
        "test_filelist=filelist[:3]"
      ],
      "metadata": {
        "id": "Vpn7H3ixwtS9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def ngrams(double_list, model):\n",
        "    \n",
        "        \n",
        "    train, vocab=padded_everygram_pipeline(2, double_list)\n",
        "    vocab=list(vocab)\n",
        "    \n",
        "    model.fit(train, vocab)\n",
        "    \n",
        "def pre_pipeline(stringed_text):\n",
        "    listed=preprocess_sent(stringed_text)\n",
        "    double=sent_to_morph(listed)\n",
        "    \n",
        "    return double"
      ],
      "metadata": {
        "id": "3FiZljJwxVpL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#파일 받아서 전체과정\n",
        "def file_to_string(filenick):\n",
        "    dummylist=[]\n",
        "    \n",
        "    with open(filenick) as f:\n",
        "        json_data=json.load(f)\n",
        "    for i in range(0, 1000):\n",
        "        try:\n",
        "        \n",
        "            kor_text=json_data[\"SJML\"][\"text\"][i][\"content\"]\n",
        "            dummylist.append(kor_text)\n",
        "        except:\n",
        "            break\n",
        "    return dummylist\n",
        "\n",
        "def whole_pipeline(filenick, given_model):\n",
        "    dummies=file_to_string(filenick)\n",
        "    \n",
        "    for dummy in dummies:\n",
        "        \n",
        "        double_list=pre_pipeline(dummy)\n",
        "        ngrams(double_list, given_model)"
      ],
      "metadata": {
        "id": "_2MXAiwgxX5T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_model=MLE(2)\n",
        "for filename in tqdm(test_filelist):\n",
        "    uniquefile=os.path.join(d_path, filename)\n",
        "    whole_pipeline(uniquefile, test_model)"
      ],
      "metadata": {
        "id": "PRmuSsjAxuEZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3eec1cc4-9a98-4f28-9eeb-c696e106fae9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 3/3 [00:02<00:00,  1.04it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "make_it_one(test_model, [\"제품\", \"기준\"])\n",
        "#이거 문제 해결. (없는 거에 대해서 어떻게 해줄지)\n",
        "#unk 문제 해결"
      ],
      "metadata": {
        "id": "OhrSqgjpyNq_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "21ab16d5-e7d4-4a60-9ecc-b9d13a2b0990"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('제품', '을'),\n",
              " '기준',\n",
              " 9.079343964611674e-05,\n",
              " ('제품', '<UNK>'),\n",
              " '기준',\n",
              " 0.00025577626685237574,\n",
              " ('제품', '의'),\n",
              " '기준',\n",
              " 1.4804205134468445e-05]"
            ]
          },
          "metadata": {},
          "execution_count": 108
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_model.generate(20)"
      ],
      "metadata": {
        "id": "wGT2rDMoysnr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"/content/gdrive/MyDrive/origin/BWSC217000049030.json\", \"r\") as f:\n",
        "\n",
        "    json_data=json.load(f)\n",
        "#print(json.dumps(json_data))\n",
        "kor_text=json_data[\"SJML\"][\"text\"][1][\"content\"]\n"
      ],
      "metadata": {
        "id": "itAyYwqGyvxN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mecab1=Mecab()\n",
        "komo1=Komoran()\n",
        "mecab1.morphs(kor_text)"
      ],
      "metadata": {
        "id": "EPmiFvyPTFcq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "morphed=mecab1.morphs(kor_text)\n",
        "type(morphed)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uchS8Y6-VDJy",
        "outputId": "47a13fce-2214-4a63-b6a6-9ab752fe4735"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "list"
            ]
          },
          "metadata": {},
          "execution_count": 124
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.util import ngrams\n",
        "text_unigrams = [ngrams(sent, 1) for sent in double]\n",
        "text_bigrams=[ngrams(sent, 2) for sent in double]\n",
        "ngram_counts=NgramCounter(text_unigrams+text_bigrams)"
      ],
      "metadata": {
        "id": "dw4c0YYHV_2N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ngram_counts[[\"어려운\"]]"
      ],
      "metadata": {
        "id": "b8XK5edqWGB6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cce971fb-6e65-402c-ffc4-8e7237e70b4a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "FreqDist({'시기': 1})"
            ]
          },
          "metadata": {},
          "execution_count": 143
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#double"
      ],
      "metadata": {
        "id": "gHNCp8734dPW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "9n5m4c4z5DVH"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}